Tera中的Leveldb
=====

Leveldb在tera中负责表格分片的存储，一个tabletnode中可能管理几个至几千个leveldb。因此，leveldb在tera中扮演着非常重要的角色，我们对leveldb进行了大量的改变和优化。

# 功能改变

原有LevelDB的功能对Tera的需求并不是完全满足，对于不满足Tera需求的部分加以改写优化。

## 1. 类bigtable数据模型支持

[todo]

## 2. 表格分片分裂与合并

[todo]

## 3. 多存储介质支持

Tera可使用的存储介质有DFS，SSD，MEM，特性各异。而tera面临的需求场景多种多样：大数据量高吞吐的链接库数据、低延时高并发的词典类数据、大数据量资源限制严格的日志数据等等。

优化：将存储介质进行扩展和更精细的分类，扩展支持多种Env，tera的表格schema中可以配置为不同的介质。不同配置下，应用不同的存储介质，以达到取各类介质所长，实现资源的有效利用。其中，DFS是所有数据的持久化存储；SSD配置为大数据量、高频访问的表格，而且可配置多种缓存机制，配合不同的垃圾收集算法，充份利用SSD机型的优势，降低带宽消耗；MEM适合小数据量，低延时（<5ms）类表格。

如果表格中存在多个LG，则可指定为不同的存储介质，针对不同列属性进行配置，可通过表格schema进行详细配置。

## 4. 单元格原子操作

Tera业务中有大量类read-modify-write需求（如链接统计），且无modify-if的需求，客户端无需获得写入前的值。这些场景对性能要求非常高（单机10万/s+、几百客户端并发执行），常规实现远远无法满足性能需求。

优化：将read-modify-write转化为write实现，客户端并没有发起实际读请求，实际的Modify操作在后台compact过程中完成，Read操作时，实时进行modify操作，返回最终结果。

tera已实现Add、Append、Write-if-absent等语义。

## 5. 嵌套LevelDB

Tera的应用中会出现同一个key被大量重复写入（同key爆发，如上文提到的Add、Append等操作），而Tera的skiplist通常设置比较大（典型值是32M），此时随机读性能急剧下降（可能下降数万倍），而且会产生很多很小的level0文件，对后续的compact及dfs非常不友好。

优化：将leveldb的内存结构实现为一个微型leveldb，内存leveldb与外部共享同一个WAL，同key爆发的场景可及时进行内存中的compact并及时清除垃圾数据、合并有效数据，非常有效的提升了随机读的性能（和普通场景达到同一水平），降低了文件的IO操作，提高了磁盘compact的效率。

# 性能及资源利用优化

当单机管理几十至几千个leveldb时，leveldb的性能及资源消耗就变得很可观，对资源的有效利用提出了更高的要求，针对不同的情况，我们对leveldb做出了很多性能及资源利用方面的优化。

## 1. WAL异步写入

Tera的很多业务场景对吞吐有很高的要求（单机1W+），而原生leveldb中WAL的写入是同步进行。Tera的持久化存储为DFS，写入延时要比本地磁盘更高，大量的时序被浪费，性能无法达到业务吞吐要求。

实现异步WAL Writer，由后台线程完成最终的LOG写入，大幅提高了单个leveldb的写入吞吐（3W+）

## 2. leveldb卸载流程优化

tera tablet的split/merge/负载均衡等操作都会涉及到大量的leveldb的卸载和重新启动。在启动过程中，Redo WAL缓慢，对带宽及dfs都带来了很大压力。而且整个的卸载和启动过程会带来一定时间的停服（极端情况10s+）。

tera对leveldb卸载过程进行两阶段优化，卸载过程中完成WAL的清除工作，这样实现了在卸载过程中大部分时间不停服，将卸载+启动总用时控制在百ms，而且简化了tablet merge逻辑，无需merge WAL。

## 3. blockcache、tablecache支持多leveldb共享

Block cache 和 table cache是leveldb中内存占用的大头，单机leveldb数目可能有很大差异（10 ~ 10000），这样的情况下单机内存的使用变化非常不稳定，这两个LRU的性能也不能有效发挥出来。

tera实现了多leveldb共享block cache和table cache，使单进程共享两个大的LRU，提高了LRU的效率，有效控制内存总使用量。这种场景下，多个leveldb可以共享文件句柄、block缓存，提高资源利用率。

## 4. leveldb后台共享线程池

原生leveldb中，单个Env只有一个后台线程，在tera中，Env是单例的，因此，后台线程会被所有leveldb共享使用，单线程的性能成为瓶颈

tera将后台线程扩展为线程池，增加定时、优先级调度，可以多leveldb共享线程池，有效提升了leveldb数目多、压力大的场景下单机性能。

## 5. MergingIterator中排序算法优化

在对表格支持中，大量依赖了MergingIterator和复杂比较器，CPU的消耗集中在复杂比较器的调用上，高压场景下超过500万次/s

将迭代器merge的算法由顺序比较优化为堆比较，将比较器的调用次数降低24%，提高了CPU的利用效率。

# 稳定性优化

tera中的leveldb持久化存储由单机变为了分布式文件系统，当leveldb运行在分布式文件系统上时，将面对分布式文件系统中不同于本地系统的大量异常，稳定性大打折扣，tera在dfs容错方面也做了大量优化。

## 1. 各类文件丢失容错

## 2. 文件操作夯住容错

## 3. 文件副本无法恢复容错
